{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from typing import Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "__name__ = 'ah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = pd.read_csv(r\"C:\\Users\\AmirHussain\\Desktop\\ANN-project\\DataSets\\train\\alphabet_dataset.txt\", header= None)\n",
    "\n",
    "tests_10 = pd.read_csv(r\"C:\\Users\\AmirHussain\\Desktop\\ANN-project\\DataSets\\test\\letter10error.txt\", header= None)\n",
    "tests_15 = pd.read_csv(r\"C:\\Users\\AmirHussain\\Desktop\\ANN-project\\DataSets\\test\\letter15error.txt\", header= None)\n",
    "tests_20 = pd.read_csv(r\"C:\\Users\\AmirHussain\\Desktop\\ANN-project\\DataSets\\test\\letter20error.txt\", header= None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  54  55  56  57  58  59  60  \\\n",
       "0  -1  -1   1   1  -1  -1  -1  -1  -1  -1  ...   1  -1   1   1   1  -1   1   \n",
       "1  -1  -1  -1   1  -1  -1  -1  -1  -1  -1  ...   1  -1  -1   1  -1  -1  -1   \n",
       "2  -1  -1  -1   1  -1  -1  -1  -1  -1  -1  ...  -1   1   1   1  -1  -1  -1   \n",
       "3   1   1   1   1   1   1  -1  -1   1  -1  ...  -1   1   1   1   1   1   1   \n",
       "4   1   1   1   1   1   1  -1   1  -1  -1  ...  -1   1   1   1   1   1   1   \n",
       "\n",
       "   61  62  63  \n",
       "0   1   1   A  \n",
       "1   1  -1   A  \n",
       "2   1   1   A  \n",
       "3   1  -1   B  \n",
       "4   1  -1   B  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = list(string.ascii_uppercase)\n",
    "outputs = {key: val for key, val in zip(letters, range(len(letters)))}\n",
    "trains[63] = trains[63].map(outputs)\n",
    "tests_10[63] = tests_10[63].map(outputs)\n",
    "tests_15[63] = tests_15[63].map(outputs)\n",
    "tests_20[63] = tests_20[63].map(outputs)\n",
    "#########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trains.drop(63, axis= 1).to_numpy()\n",
    "X_10 = tests_10.drop(63, axis= 1).to_numpy()\n",
    "X_15 = tests_15.drop(63, axis= 1).to_numpy()\n",
    "X_20 = tests_20.drop(63, axis= 1).to_numpy()\n",
    "################################\n",
    "Y_train = trains[63].to_numpy()\n",
    "Y_train = Y_train.tolist()\n",
    "\n",
    "Y_10 = tests_10[63].to_numpy()\n",
    "Y_10 = Y_10.tolist()\n",
    "\n",
    "Y_15 = tests_15[63].to_numpy()\n",
    "Y_15 = Y_15.tolist()\n",
    "\n",
    "Y_20 = tests_20[63].to_numpy()\n",
    "Y_20 = Y_20.tolist()\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    def __init__(self, rows, cols, data):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.data = data\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{str(self.data)}\"\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{repr(self.data)}\"\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    @classmethod\n",
    "    def zeros(cls, rows, cols):\n",
    "        data = [[0.0] * cols for _ in range(rows)]\n",
    "        return cls(rows, cols, data)\n",
    "\n",
    "    @classmethod\n",
    "    def random(cls, rows, cols):\n",
    "        data = cls.zeros(rows, cols)\n",
    "        for i in range(len(data.data)):\n",
    "            for j in range(len(data.data[i])):\n",
    "                data.data[i][j] = random.uniform(-1, 1)\n",
    "        return cls(rows, cols, data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(cls, data):\n",
    "        rows = len(data)\n",
    "        cols = len(data[0])\n",
    "        for row in range(1, rows):\n",
    "            if len(data[row - 0]) != len(data[row]): \n",
    "                raise ValueError(f\"All rows must have the same number of columns dif in {data[row - 0]},{data[row]}\")\n",
    "        return cls(rows, cols, data)\n",
    "\n",
    "    def multiply(self, other):\n",
    "        if not isinstance(other, Matrix):\n",
    "            other = Matrix.from_data(other)\n",
    "        if self.cols != other.rows:\n",
    "            raise ValueError(f\"Attempted to multiply by matrix of incorrect dimensions self=({self.rows},{other.rows}) other=({self.cols},{other.cols})\")\n",
    "\n",
    "        res_data = [[0.0] * other.cols for _ in range(self.rows)]\n",
    "\n",
    "        for i in range(self.rows):\n",
    "            for j in range(other.cols):\n",
    "                sum_val = sum(self.data[i][k] * other.data[k][j] for k in range(self.cols))\n",
    "                res_data[i][j] = sum_val\n",
    "\n",
    "        return Matrix(self.rows, other.cols, res_data)\n",
    "\n",
    "    def add(self, other):\n",
    "        if self.rows != other.rows or self.cols != other.cols:\n",
    "            raise ValueError(f\"Attempted to add matrix of incorrect dimensions; self=({self.rows},{other.rows}) other=({self.cols},{other.cols})\")\n",
    "\n",
    "        res_data = [[self.data[i][j] + other.data[i][j] for j in range(self.cols)] for i in range(self.rows)]\n",
    "\n",
    "        return Matrix(self.rows, self.cols, res_data)\n",
    "\n",
    "    def dot_multiply(self, other):\n",
    "        if self.rows != other.rows or self.cols != other.cols:\n",
    "            raise ValueError(f\"Attempted to dot multiply by matrix of incorrect dimensions; self=({self.rows},{other.rows}) other=({self.cols},{other.cols})\")\n",
    "\n",
    "        res_data = [[self.data[i][j] * other.data[i][j] for j in range(self.cols)] for i in range(self.rows)]\n",
    "\n",
    "        return Matrix(self.rows, self.cols, res_data)\n",
    "\n",
    "    def subtract(self, other):\n",
    "        if self.rows != other.rows or self.cols != other.cols:\n",
    "            raise ValueError(f\"Attempted to subtract matrix of incorrect dimensions; self=({self.rows},{other.rows}) other=({self.cols},{other.cols})\")\n",
    "\n",
    "        res_data = [[self.data[i][j] - other.data[i][j] for j in range(self.cols)] for i in range(self.rows)]\n",
    "\n",
    "        return Matrix(self.rows, self.cols, res_data)\n",
    "    \n",
    "    def map(self, function):\n",
    "        res_data = [[function(value) for value in row] for row in self.data]\n",
    "        return Matrix(self.rows, self.cols, res_data)\n",
    "\n",
    "    def transpose(self):\n",
    "        res_data = np.array(self.data).transpose().tolist()\n",
    "        # res_data = [[self.data[j][i] for j in range(self.cols)] for i in range(self.rows)]\n",
    "        return Matrix(self.cols, self.rows, res_data)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def tolist(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actiovation:\n",
    "    def __init__(self, func : callable, derivative: callable) -> None:\n",
    "        self.func : callable = func\n",
    "        self.derivative : callable = derivative\n",
    "    def __str__(self) -> str:\n",
    "        return f\"function   = {str(self.func)},\\nderivation = {str(self.derivative)}\"\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__str__()}\"\n",
    "\n",
    "\n",
    "def Bipolar_sigmoid_function(x):\n",
    "    \"\"\"Bipolar sigmoid function\"\"\"\n",
    "    if x >= 0:\n",
    "        return (1 / (1 + np.exp(-2 * x))) - 0.5\n",
    "    else:\n",
    "        return -(1 / (1 + np.exp(2 * x))) - 0.5\n",
    "\n",
    "def Bipolar_sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the bipolar sigmoid function\"\"\"\n",
    "    if x >= 0:\n",
    "        return 4 * x * ((np.exp(-2 * x))/(1 + np.exp(-2 * x)))**2\n",
    "    else:\n",
    "        return -4 * x * ((np.exp(2 * x))/(1 + np.exp(2 * x)))**2\n",
    "\n",
    "\n",
    "def Sigmoid_function(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def Sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the sigmoid function\"\"\"\n",
    "    return Sigmoid_function(x)*(1-Sigmoid_function(x))\n",
    "\n",
    "def Relu_function(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return max(0,x)\n",
    "\n",
    "def Relu_derivative(x):\n",
    "    \"\"\"Derivative of the ReLU activation function\"\"\"\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x <= 0:\n",
    "        return 0\n",
    "    \n",
    "def Tanh_function(x):\n",
    "    \"\"\"Tanh activation function\"\"\"\n",
    "    return np.tanh(x)\n",
    "def Tanh_derivative(x):\n",
    "    \"\"\"Derivative of the tanh activation function\"\"\"\n",
    "    return 1-(np.tanh(x))**2\n",
    "\n",
    "def Softmax_function(x):\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x/e_x.sum()\n",
    "\n",
    "def Softmax_derivative(x):\n",
    "    \"\"\"Derivative of softmax activation function\"\"\"\n",
    "    return Softmax_function(x)*(1-Softmax_function(x))\n",
    "\n",
    "def Linear_function(x):\n",
    "    \"\"\"Linear activation function\"\"\"\n",
    "    return x\n",
    "def Linear_derivative(x):\n",
    "    \"\"\"Derivative of linear activation function\"\"\"\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "# Activations functions and their derivatives\n",
    "relu_activator: Actiovation = Actiovation(\n",
    "                                        func = Relu_function,\n",
    "                                        derivative = Relu_derivative)\n",
    "sigmode_activator: Actiovation = Actiovation(\n",
    "                                        func   =  Sigmoid_function ,\n",
    "                                        derivative =  Sigmoid_derivative )\n",
    "Bipolar_sigmoid_activator : Actiovation = Actiovation(\n",
    "                                        func = Bipolar_sigmoid_function,\n",
    "                                        derivative = Bipolar_sigmoid_derivative)\n",
    "linear_activator : Actiovation = Actiovation(\n",
    "                                        func = Linear_function,\n",
    "                                        derivative = Linear_derivative)\n",
    "softmax_activator : Actiovation = Actiovation(\n",
    "                                        func = Softmax_function,\n",
    "                                        derivative = Softmax_derivative)\n",
    "tanh_activator : Actiovation = Actiovation(\n",
    "                                        func = Tanh_function,\n",
    "                                        derivative = Tanh_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetWork:\n",
    "    def __type__(self):\n",
    "        return \"NLP\"\n",
    "    def __init__(self, \n",
    "                 layers : tuple[int] = (100,), \n",
    "                 lr : float = 0.00001, \n",
    "                 max_iter : int = 200,\n",
    "                 activation: Literal[\"bisig\", \"sigmoide\", \n",
    "                                     \"relu\", 'tanh', 'softmax'] = \"bisig\" , \n",
    "                 ) -> None:\n",
    "        if isinstance(layers, tuple):\n",
    "            layers = list(layers)\n",
    "        if 0. > lr > 1.:\n",
    "            raise ValueError(\"Learning rate must be between 0 and 1\")\n",
    "\n",
    "        if not (isinstance(layers, tuple) or isinstance(layers, list)):\n",
    "            raise TypeError(\"layers input should be a tuple/list of Integers\")\n",
    " \n",
    "        self.lr : float = lr\n",
    "        self.layers : tuple[int,] = layers\n",
    "        self.activation = activation\n",
    "        self.max_iter : int = max_iter\n",
    "\n",
    "        match self.activation:\n",
    "            case \"bisig\":\n",
    "                self.activation_function = Bipolar_sigmoid_activator \n",
    "            case \"sigmoide\":\n",
    "                self.activation_function = sigmode_activator\n",
    "            case \"relu\":\n",
    "                self.activation_function = relu_activator\n",
    "            case \"tanh\":\n",
    "                self.activation_function = tanh_activator\n",
    "            case \"softmax\":\n",
    "                self.activation_function = softmax_activator\n",
    "            case _:\n",
    "                raise ValueError('Invalid activation Function')\n",
    "            \n",
    "    def __str__(self) -> str:\n",
    "        return f\"MLPNetWork(lr = {self.lr}, hidden_layers = {tuple(self.layers)},  iteration= {self.max_iter}, activator={str(self.activation)} \"\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__str__()}\"\n",
    "    \n",
    "    def _build_weights_and_bias(self, layers):\n",
    "        weights = list()\n",
    "        bias = list()\n",
    "        # [63, 10, 26]\n",
    "        for i in range(len(layers) - 1 ):\n",
    "            weights.append(Matrix.random( rows= layers[i + 1], cols=  layers[i] )  )\n",
    "            bias.append(Matrix.random(rows= layers[i + 1], cols= 1 )  )\n",
    "\n",
    "        return weights, bias\n",
    "\n",
    "    def _feed_forward(self, inputs: list):\n",
    "        if len(inputs) != self.layers[0]:\n",
    "            raise customError(\"Invalid inputs length\")\n",
    "        if not isinstance(inputs, list):\n",
    "            inputs = list(inputs)\n",
    "\n",
    "        current = Matrix.from_data([inputs]).transpose()\n",
    "        self.data = [current]\n",
    "        for layer in range(len(self.layers) -1):\n",
    "            # print(self.layers)\n",
    "            current = self.weights[layer].\\\n",
    "                    multiply(current).\\\n",
    "                    add(self.bias[layer]).\\\n",
    "                    map(self.activation_function.func)\n",
    "            \n",
    "            self.data.append(current)\n",
    "\n",
    "        return current.transpose().data[0]\n",
    "        \n",
    "        \n",
    "    def _back_propogate(self, outputs, targets):\n",
    "        if len(targets) != self.layers[len(self.layers) - 1 ]:\n",
    "            raise customError(\"Invalid targets length\")\n",
    "\n",
    "        paresd  = Matrix.from_data([outputs]).transpose()\n",
    "        err = Matrix.from_data([targets])\\\n",
    "                                .transpose()\\\n",
    "                                    .subtract(paresd)\n",
    "        \n",
    "        gradient = paresd.map(self.activation_function.derivative)\n",
    "\n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            # print(i)\n",
    "            gradient = gradient.dot_multiply(err).\\\n",
    "                map(lambda x : x * self.lr)\n",
    "            \n",
    "            self.weights[i] = self.weights[i].\\\n",
    "                add (gradient.\\\n",
    "                     multiply( self.data[i].transpose() ) )\n",
    "            \n",
    "            self.bias[i] = self.bias[i] .add(gradient)\n",
    "\n",
    "            err = self.weights[i].transpose().multiply (err)\n",
    "            gradient =  self.data[i].map(self.activation_function.derivative)\n",
    "        \n",
    "\n",
    "    def fit(self, inputs, targets : list):\n",
    "        \"\"\"Train the model with given data and target values\"\"\"\n",
    "        s = len(set(targets ))\n",
    "        perfect_layer = self.layers\n",
    "        perfect_layer.insert(0, len(inputs[0]))\n",
    "        perfect_layer.append(s)\n",
    "        \n",
    "        self.weights , self.bias = self._build_weights_and_bias(perfect_layer)\n",
    "\n",
    "        target = [ [0.] * s for _ in range(len(targets)) ]\n",
    " \n",
    "        for index, value in enumerate(target):\n",
    "            value[int(targets[index])] = 1.\n",
    "\n",
    "        \n",
    "        for _ in tqdm(range(self.max_iter)):\n",
    "            for j in range(len(inputs)):\n",
    "                out = self._feed_forward(inputs[j])\n",
    "                self._back_propogate(out, target[j])\n",
    "\n",
    "        return self.__repr__()\n",
    "\n",
    "            \n",
    "\n",
    "    def predict(self, inputs):\n",
    "        res = list()\n",
    "        \n",
    "        try:\n",
    "            len(inputs[0])\n",
    "            for i in inputs:\n",
    "                outputs = self._feed_forward(i)\n",
    "                \n",
    "                res.append(np.array(outputs).tolist())\n",
    "            \n",
    "            # print(\"Res outputs \", outputs)\n",
    "            # print(res)\n",
    "            predicted = [ np.argmax(idx  ) for idx in res]\n",
    "            # print(predicted)\n",
    "            return predicted\n",
    "        except:\n",
    "            output = self._feed_forward(inputs)\n",
    "            # print(\"output \",output)\n",
    "\n",
    "            predicted = [ np.argmax(idx  ) for idx in output]\n",
    "            # print(predicted)\n",
    "            return predicted\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        y_hat = np.array(self.predict(x_test))\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        correct_predictions = np.sum(y_hat == y_test)\n",
    "        total_samples = len(y_test)\n",
    "        \n",
    "        accuracy = correct_predictions / total_samples\n",
    "        return accuracy\n",
    "                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:20<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "MLPNetWork(lr = 0.5, hidden_layers = (63, 100, 26),  iteration= 100, activator=sigmoide \n",
      "MLPClassifier(activation='logistic', alpha=0.5, max_iter=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 , 15 , 20, 25, 30\n",
    "Hidden_Layer : tuple[int,] = ( 100, )\n",
    "print(type(Hidden_Layer))\n",
    "# .1 , .2 , .3 , .4 , .5 , .6 , .7 , .8 , .9\n",
    "learning_rate : float = .5\n",
    "# 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000\n",
    "iteration : int = 100\n",
    "active_function : Literal['bisig', 'sigmoide', 'relu', 'tanh', 'softmax'] = 'sigmoide'\n",
    "\n",
    "my_model = MLPNetWork(layers= Hidden_Layer,\n",
    "                      lr = learning_rate,\n",
    "                      max_iter = iteration,\n",
    "                      activation = active_function)\n",
    "my_model_history = my_model.fit(X_train ,Y_train)\n",
    "\n",
    "\n",
    "sklearn_model = MLPClassifier(hidden_layer_sizes= Hidden_Layer,\n",
    "                              activation = 'logistic',\n",
    "                              alpha= learning_rate,\n",
    "                              max_iter= iteration)\n",
    "\n",
    "sklearn_model_history = sklearn_model.fit(X_train, Y_train)\n",
    "\n",
    "print(\"-\"*21)\n",
    "print(my_model_history)\n",
    "print(sklearn_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Model Evaluation on X_10:  0.62\n",
      "My Model Evaluation on X_15:  0.65\n",
      "My Model Evaluation on X_20:  0.5\n",
      "==================================\n",
      "Sklearn Evaluation on X_10:   0.65\n",
      "Sklearn Evaluation on X_15:   0.58\n",
      "Sklearn Evaluation on X_20:   0.58\n"
     ]
    }
   ],
   "source": [
    "# My Model\n",
    "my_eval_10 =  my_model.evaluate(X_10, Y_10)\n",
    "my_eval_15 =  my_model.evaluate(X_15, Y_15)\n",
    "my_eval_20 =  my_model.evaluate(X_20, Y_20)\n",
    "# Sklearn Model\n",
    "sklearn_eval_10 =  sklearn_model.score(X_10, Y_10)\n",
    "sklearn_eval_15 =  sklearn_model.score(X_15, Y_15)\n",
    "sklearn_eval_20 =  sklearn_model.score(X_20, Y_20)\n",
    "\n",
    "# print Results of evals\n",
    "print(\"My Model Evaluation on X_10: \" , round(my_eval_10, 2)  )\n",
    "print(\"My Model Evaluation on X_15: \" , round(my_eval_15, 2)  )\n",
    "print(\"My Model Evaluation on X_20: \" , round(my_eval_20, 2)  )\n",
    "#################################\n",
    "print(\"=\" *34)\n",
    "#################################\n",
    "print(\"Sklearn Evaluation on X_10:  \" , round(sklearn_eval_10, 2) )\n",
    "print(\"Sklearn Evaluation on X_15:  \" , round(sklearn_eval_15, 2) )\n",
    "print(\"Sklearn Evaluation on X_20:  \" , round(sklearn_eval_20, 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My prediction on X_10 :     [0, 3, 16, 3, 5, 5, 8, 3, 19, 19, 10, 11, 15, 13, 16, 5, 8, 17, 6, 19, 16, 0, 7, 23, 24, 24]\n",
      "The true value of Y_10 is:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "__________________________________\n",
      "My prediction on X_10 :     [0, 3, 5, 14, 4, 5, 8, 21, 19, 19, 10, 11, 7, 13, 8, 3, 14, 7, 8, 19, 16, 24, 12, 23, 24, 24]\n",
      "The true value of Y_10 is:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "__________________________________\n",
      "My prediction on X_10 :     [0, 3, 16, 14, 18, 5, 8, 7, 19, 19, 10, 11, 10, 13, 16, 17, 8, 4, 5, 19, 6, 23, 7, 23, 8, 23]\n",
      "The true value of Y_10 is:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f\"My prediction on X_10 :     {my_model.predict(X_10)}\")\n",
    "print(f\"The true value of Y_10 is:  {Y_10}\")\n",
    "print(\"_\" *34)\n",
    "\n",
    "print(f\"My prediction on X_10 :     {my_model.predict(X_15)}\")\n",
    "print(f\"The true value of Y_10 is:  {Y_15}\")\n",
    "print(\"_\" *34)\n",
    "\n",
    "print(f\"My prediction on X_10 :     {my_model.predict(X_20)}\")\n",
    "print(f\"The true value of Y_10 is:  {Y_20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction on X_10 : [0, 18, 2, 3, 4, 5, 16, 7, 19, 19, 10, 11, 23, 13, 14, 5, 16, 18, 18, 19, 20, 24, 7, 23, 8, 25]\n",
      "The true value of Y_10 is:   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "__________________________________\n",
      "Sklearn prediction on X_10 : [0, 1, 2, 3, 4, 5, 14, 7, 19, 19, 10, 11, 22, 13, 16, 5, 16, 15, 18, 19, 3, 21, 0, 23, 24, 25]\n",
      "The true value of Y_10 is:   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "__________________________________\n",
      "Sklearn prediction on X_10 : [0, 18, 2, 3, 4, 5, 16, 7, 19, 19, 10, 11, 18, 13, 2, 5, 16, 5, 18, 19, 11, 23, 1, 23, 24, 25]\n",
      "The true value of Y_10 is:   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sklearn prediction on X_10 : {sklearn_model.predict(X_10).tolist()}\")\n",
    "print(f\"The true value of Y_10 is:   {Y_10}\")\n",
    "print(\"_\" *34)\n",
    "\n",
    "print(f\"Sklearn prediction on X_10 : {sklearn_model.predict(X_15).tolist()}\")\n",
    "print(f\"The true value of Y_10 is:   {Y_15}\")\n",
    "print(\"_\" *34)\n",
    "\n",
    "print(f\"Sklearn prediction on X_10 : {sklearn_model.predict(X_20).tolist()}\")\n",
    "print(f\"The true value of Y_10 is:   {Y_20}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
